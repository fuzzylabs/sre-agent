# SRE Agent Full Configuration
# This file contains all possible configuration options for production use
# All features enabled - requires credentials for all services

# ===== ESSENTIAL CREDENTIALS =====
# Basic authentication token for API access
DEV_BEARER_TOKEN=your_dev_token_here

# Hugging Face token for Llama Firewall (REQUIRED)
# Get from: https://huggingface.co/settings/tokens
# Needs read access to meta-llama/Llama-Prompt-Guard-2-86M
HF_TOKEN=your_hugging_face_token_here

# LLM Provider Configuration
PROVIDER=anthropic  # Options: anthropic, gemini, mock
MODEL=claude-3-5-sonnet-20241022  # Your preferred model

# LLM API Keys (provide the one matching your PROVIDER)
ANTHROPIC_API_KEY=your_anthropic_api_key_here  # Required if PROVIDER=anthropic
GEMINI_API_KEY=your_gemini_api_key_here        # Required if PROVIDER=gemini

# ===== SLACK INTEGRATION =====
# For sending notifications and alerts
SLACK_BOT_TOKEN=your_slack_bot_token_here
SLACK_TEAM_ID=your_slack_team_id_here
SLACK_SIGNING_SECRET=your_slack_signing_secret_here
SLACK_CHANNEL_ID=your_slack_channel_id_here

# ===== GITHUB INTEGRATION =====
# For repository access and code analysis
GITHUB_PERSONAL_ACCESS_TOKEN=your_github_token_here
GITHUB_ORGANISATION=your_org_name
GITHUB_REPO_NAME=your_repo_name
PROJECT_ROOT=src  # Root directory of your project

# ===== KUBERNETES INTEGRATION =====
# Choose AWS (EKS) OR GCP (GKE) - not both

# AWS EKS Configuration
AWS_REGION=us-east-1
AWS_ACCOUNT_ID=your_aws_account_id
TARGET_EKS_CLUSTER_NAME=your_eks_cluster_name

# GCP GKE Configuration (alternative to AWS)
# CLOUDSDK_CORE_PROJECT=your_gcp_project_id
# CLOUDSDK_COMPUTE_REGION=us-central1
# TARGET_GKE_CLUSTER_NAME=your_gke_cluster_name

# ===== SERVICE CONFIGURATION =====
# Services running on your cluster (customize for your setup)
SERVICES=["cartservice", "adservice", "emailservice", "frontend", "checkoutservice"]

# Tools available to the agent
TOOLS=["list_pods", "get_logs", "get_file_contents", "slack_post_message", "create_github_issue"]

# ===== PERFORMANCE SETTINGS =====
# Maximum tokens the LLM can generate
MAX_TOKENS=10000

# Query timeout in seconds
QUERY_TIMEOUT=300

# ===== USAGE =====
# 1. Replace all placeholder values with your actual credentials
# 2. Choose either AWS or GCP configuration (comment out the unused one)
# 3. Customize SERVICES and TOOLS for your environment
# 4. Start with:
#    - AWS: docker compose -f compose.aws.yaml up
#    - GCP: docker compose -f compose.gcp.yaml up
# 5. Access the API at http://localhost:8003